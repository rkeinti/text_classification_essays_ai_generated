{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac45b43e",
   "metadata": {},
   "source": [
    "## TextVectorization in the model itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2ac8565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense, Embedding, Conv1D, GlobalMaxPooling1D, Dropout, MultiHeadAttention, LayerNormalization, Input\n",
    "from keras.layers import TextVectorization\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfTransformer,TfidfVectorizer,CountVectorizer\n",
    "#from keras_nlp.layers import TokenAndPositionEmbedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.metrics import roc_curve,roc_auc_score,accuracy_score\n",
    "from keras import Model\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, GRU, SpatialDropout1D\n",
    "from gensim.parsing.preprocessing import preprocess_string, strip_tags, strip_punctuation, strip_numeric, remove_stopwords, strip_short, stem_text\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f86dd7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the input files\n",
    "train = pd.read_csv('data/train_essays.csv')\n",
    "additional_essays = pd.read_csv('data/additional_generated_essays.csv')\n",
    "kaggle_essays = pd.read_csv('data/ai_generated_train_essays.csv')\n",
    "kaggle_essays_gpt = pd.read_csv('data/ai_generated_train_essays_gpt-4.csv')\n",
    "prompt_id = pd.read_csv('data/train_prompts.csv')\n",
    "test_df = pd.read_csv('data/test_essays.csv')\n",
    "df_zach = pd.read_csv('data/zach_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2003825e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging the datasets for training\n",
    "train_c_n = pd.concat([train.drop(['id'], axis=1), additional_essays\n",
    "                       ,kaggle_essays.drop(['id'], axis=1)\n",
    "                       ,kaggle_essays_gpt.drop(['id'], axis=1), df_zach], ignore_index=True)\n",
    "#train_c_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "242c3651",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_zach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94334f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# origal training data plus additional data for training\n",
    "train_data = train_c_n['text']\n",
    "train_labels = train_c_n['generated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acf2e7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "vocab_size = 30000 # Only consider the top 20k words\n",
    "maxlen = 1024 # Only consider the first 200 words of each movie review\n",
    "embed_dim = 64 # Embedding size for each token\n",
    "num_heads = 4 # Number of attention heads\n",
    "ff_dim = 32 # Hidden layer size in feed forward network inside transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e4d35ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    x=remove_stopwords(text)\n",
    "    x=strip_punctuation(x)\n",
    "    x=strip_numeric(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "027d7099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2min 2s\n",
      "Wall time: 2min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_data = train_data.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6f587c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer = TextVectorization(output_mode='int', output_sequence_length=maxlen, max_tokens=vocab_size, ngrams=(2,3,4,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8dfd215",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_data, train_labels, test_size=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4610d6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data_tf = tf.data.Dataset.from_tensor_slices(X_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ae84883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4min 21s\n",
      "Wall time: 6min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#vectorize_layer.adapt(train_data_tf.batch(512))\n",
    "vectorize_layer.adapt(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "053338e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_data, train_labels, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5ce642f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_data, train_c_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94436ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2071"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5637f27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define TransformerEncoder layer\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "124db5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "#        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5447badb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5ae383c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "text_input = Input(shape=(1,), dtype=tf.string, name='text')\n",
    "#embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "#x = Embedding(X_p.shape[1], embed_dim, input_length=maxlen)(inputs)\n",
    "x = vectorize_layer(text_input)\n",
    "x = Embedding(vocab_size+1, embed_dim, mask_zero=True)(x)\n",
    "x = SpatialDropout1D(0.2)(x)\n",
    "transformer_block = TransformerEncoder(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "#x = Bidirectional(LSTM(64, return_sequences=True))(x)\n",
    "x = Bidirectional(GRU(64))(x)\n",
    "#x = Bidirectional(LSTM(32, dropout=0.2))(x)\n",
    "x = Dropout(0.1)(x)\n",
    "#x = GlobalMaxPooling1D()(x)\n",
    "outputs = Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "model = Model(inputs=text_input, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a4fc65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text (InputLayer)            [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "text_vectorization (TextVect (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 1024, 64)          1920064   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d (SpatialDr (None, 1024, 64)          0         \n",
      "_________________________________________________________________\n",
      "transformer_encoder (Transfo (None, 1024, 64)          70816     \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 128)               49920     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 2,041,058\n",
      "Trainable params: 2,041,058\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=1e-3), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74fa34ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping to prevent overfitting\n",
    "#early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4eebb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = 'tmp/chkpt'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97b76573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "22256/22256 [==============================] - 3638s 163ms/step - loss: 0.2717 - accuracy: 0.8720 - val_loss: 0.2233 - val_accuracy: 0.8988\n",
      "Epoch 2/2\n",
      "22256/22256 [==============================] - 3291s 148ms/step - loss: 0.1969 - accuracy: 0.9139 - val_loss: 0.1972 - val_accuracy: 0.9152\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=2, validation_data=(X_val, y_val), callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb31f0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26aabcc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9751326028038397"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_score = roc_auc_score(y_val, y_pred[:,1])\n",
    "auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd491266",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_sub = test_df['text']\n",
    "X_test_sub = X_test_sub.apply(clean_text)\n",
    "proba_ = model.predict(X_test_sub)\n",
    "test_df['generated']=proba_[:,1]\n",
    "test_df_submission = test_df.drop(['prompt_id','text'], axis=1).copy()\n",
    "test_df_submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b35ed97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000aaaa</td>\n",
       "      <td>0.597391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1111bbbb</td>\n",
       "      <td>0.597391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2222cccc</td>\n",
       "      <td>0.597391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  generated\n",
       "0  0000aaaa   0.597391\n",
       "1  1111bbbb   0.597391\n",
       "2  2222cccc   0.597391"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbb8b84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
